{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99b650e5",
   "metadata": {},
   "source": [
    "## KNeighbors Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8946a7",
   "metadata": {},
   "source": [
    "This notebook is for the KNeighbors Regression on the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b1589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload # this ensures modules are reloaded automatically\n",
    "# %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8138c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "from IPython.display import display\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cleaning methods\n",
    "import cleaning\n",
    "import constants\n",
    "import imputation\n",
    "import auxiliary\n",
    "\n",
    "# Scikit learn\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "from matplotlib.patches import Rectangle, Circle\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# For adaboost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8565477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main dataset\n",
    "df_prices_train = pd.read_csv(\"data/train.csv\")\n",
    "df_prices_train_demo = pd.read_csv(\"data/train.csv\")\n",
    "df_prices_test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "# Have a peek at the data\n",
    "print(\"Property Prices train dataset\", df_prices_train.shape)\n",
    "display(df_prices_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa82264c",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94c896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take 4+1 as 5\n",
    "df_prices_train['bedrooms'] = df_prices_train['bedrooms'].apply(cleaning.process_bedroom_sum)\n",
    "df_prices_test['bedrooms'] = df_prices_test['bedrooms'].apply(cleaning.process_bedroom_sum) \n",
    "\n",
    "# Take 4+1 as 4.5\n",
    "# df_prices_train['bedrooms'] = df_prices_train['bedrooms'].apply(cleaning.process_bedroom_half)\n",
    "# df_prices_test['bedrooms'] = df_prices_test['bedrooms'].apply(cleaning.process_bedroom_half) \n",
    "df_prices_train[df_prices_train['bedrooms'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790e54d6",
   "metadata": {},
   "source": [
    "## Add Auxiliary Data (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c21203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import auxiliary data\n",
    "df_commercial_centres = pd.read_csv(\"data/auxiliary-data/auxiliary-data/sg-commerical-centres.csv\")\n",
    "df_train_stations = pd.read_csv(\"data/auxiliary-data/auxiliary-data/sg-train-stations.csv\")\n",
    "df_shopping_malls = pd.read_csv(\"data/auxiliary-data/auxiliary-data/sg-shopping-malls.csv\")\n",
    "df_primary_schools = pd.read_csv(\"data/auxiliary-data/auxiliary-data/sg-primary-schools.csv\")\n",
    "df_secondary_schools = pd.read_csv(\"data/auxiliary-data/auxiliary-data/sg-secondary-schools.csv\")\n",
    "\n",
    "# Have a peek at the data\n",
    "print(\"Aux: Commercial Centres\", df_commercial_centres.shape)\n",
    "display(df_commercial_centres.head())\n",
    "print(\"Aux: Train Stations\", df_train_stations.shape)\n",
    "display(df_train_stations.head())\n",
    "print(\"Aux: Shopping Malls\", df_shopping_malls.shape)\n",
    "display(df_shopping_malls.head())\n",
    "print(\"Aux: Primary Schools\", df_primary_schools.shape)\n",
    "display(df_primary_schools.head())\n",
    "print(\"Aux: Secondary Schools\", df_secondary_schools.shape)\n",
    "display(df_secondary_schools.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79854d28",
   "metadata": {},
   "source": [
    "We add some helper functions for computing the distances to these important locations and add the distances to our dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5130eccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy import distance\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Helper Functions \n",
    "def distance_ll(lat1: float, lng1: float, lat2: float, lng2: float) -> float:\n",
    "    return distance.distance((lat1, lng1), (lat2, lng2))\n",
    "\n",
    "def dist_to_nearest_item(expr: pd.DataFrame, aux_df: pd.DataFrame) -> str:\n",
    "    return round(min(aux_df.apply(lambda x: distance_ll(expr['lat'], expr['lng'], x['lat'], x['lng']), axis=1)).km, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad54761b",
   "metadata": {},
   "source": [
    "## KNeighbors Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91e4b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "X_train = df_prices_train.loc[:, df_prices_train.columns != 'price']\n",
    "y_train = df_prices_train[['price']].values.ravel()\n",
    "print(X_train.shape)\n",
    "model = KNeighborsRegressor()\n",
    "# Set up pipeline with imputer for proper grid search\n",
    "# Reference: https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html\n",
    "pipeline = Pipeline(steps=[('imputer', KNNImputer()), ('model', model)])\n",
    "\n",
    "# define grid search for hyperparameters\n",
    "grid = {\n",
    "    'imputer__n_neighbors': [3, 5, 7, 11, 15],\n",
    "    'model__n_neighbors': [i for i in range(1, 32, 2)]\n",
    "}\n",
    "\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=1, random_state=1)\n",
    "# define grid search for hyperparameters\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "mse = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "grid_search = HalvingGridSearchCV(estimator=pipeline, param_grid=grid, n_jobs=-1, cv=cv, scoring=mse, verbose=1)\n",
    "# Execute the grid search\n",
    "grid_results = grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65cef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get RMSE of grid results best score\n",
    "best_rmse = (-grid_results.best_score_) ** 0.5\n",
    "print(\"Best: %f using %s\" % (best_rmse, grid_results.best_params_))\n",
    "means = grid_results.cv_results_['mean_test_score']\n",
    "stds = grid_results.cv_results_['std_test_score']\n",
    "params = grid_results.cv_results_['params']\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6766ff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "clf = grid_results\n",
    "X_test = df_prices_test.loc[:, df_prices_test.columns != 'price']\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3b0a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame({\"Predicted\": y_pred})\n",
    "result.index.name = \"Id\"\n",
    "result.to_csv(\"submission_kneighbors.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
